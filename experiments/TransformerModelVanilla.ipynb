{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transformer Network for Text Summarization</b>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFh-7-Viy0GL"
   },
   "outputs": [],
   "source": [
    "# global installs\n",
    "!pip install datasets rouge transformers torch sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Will run on device: %s' % (device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Rouge evaluation funciton </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "rouge_ = Rouge()\n",
    "\n",
    "def print_rouge_score(generated, reference):\n",
    "    l_rouge = []\n",
    "    \n",
    "    for row in tqdm(rouge_.get_scores(generated, reference)):\n",
    "        l_rouge.append([[row['rouge-1']['r'], row['rouge-1']['p'], row['rouge-1']['f']], \n",
    "                        [row['rouge-2']['r'], row['rouge-2']['p'], row['rouge-2']['f']],\n",
    "                        [row['rouge-l']['r'], row['rouge-l']['p'], row['rouge-l']['f']]])\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print('rouge-1: r:%2.2f, p:%2.2f f:%2.2f' % (np.mean(l_rouge[0][0]), np.mean(l_rouge[0][1]), np.mean(l_rouge[0][2])))\n",
    "    print('rouge-2: r:%2.2f, p:%2.2f f:%2.2f' % (np.mean(l_rouge[1][0]), np.mean(l_rouge[1][1]), np.mean(l_rouge[1][2])))\n",
    "    print('rouge-l: r:%2.2f, p:%2.2f f:%2.2f' % (np.mean(l_rouge[2][0]), np.mean(l_rouge[2][1]), np.mean(l_rouge[2][2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Transformer Architecture Components and Implementation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PostionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "        k_t = k.transpose(2, 3)  # transpose\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score)\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        # self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.pos_emb = PostionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        # TODO : we should implement visualization\n",
    "        return out\n",
    "\n",
    "    def split(self, tensor):\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        return tensor\n",
    "\n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=s_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=t_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, \n",
    "                 enc_voc_size, dec_voc_size, d_model, n_head, max_len, ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src)\n",
    "        src_trg_mask = self.make_pad_mask(trg, src)\n",
    "        trg_mask = self.make_pad_mask(trg, trg) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    def make_pad_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "        q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(tgt, memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model lifecycle functions definition </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tokenizer, model, iterator, optimizer, criterion, clip, max_len):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        sample_list = list(zip(batch['article'], batch['highlights']))\n",
    "        for article, highlight in sample_list:\n",
    "            src = tokenizer.encode(article, max_length=max_len, return_tensors=\"pt\", truncation=True).to(device)\n",
    "            trg = tokenizer.encode(highlight, max_length=max_len, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, model, iterator, criterion, max_len):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_rouge_score = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "            sample_list = list(zip(batch['article'], batch['highlights']))\n",
    "            total_rouge_score = []\n",
    "            for article, highlight in sample_list:\n",
    "                src = tokenizer.encode(article, max_length=max_len, return_tensors=\"pt\", truncation=True).to(device)\n",
    "                trg = tokenizer.encode(highlight, max_length=max_len, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "                output = model(src, trg[:, :-1])\n",
    "                \n",
    "                output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "                trg = trg[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(output_reshape, trg)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                output_words = output.max(dim=2).indices[0]\n",
    "                output_words = tokenizer.decode(output_words)\n",
    "                rouge_score = rouge_.get_scores(output_words, highlight)[0]\n",
    "                total_rouge_score.append([rouge_score['rouge-1']['f'], \n",
    "                                          rouge_score['rouge-2']['f'], \n",
    "                                          rouge_score['rouge-l']['f']])\n",
    "                \n",
    "            r1, r2, rl = 0, 0, 0\n",
    "            for a in total_rouge_score:\n",
    "                r1 += a[0]\n",
    "                r2 += a[1]\n",
    "                rl += a[2]\n",
    "            total = len(total_rouge_score)\n",
    "            if total == 0:\n",
    "                total_rouge_score.append([0,0,0])\n",
    "            else:\n",
    "                batch_rouge_score.append([r1 / total, \n",
    "                                          r2 / total, \n",
    "                                          rl / total])\n",
    "    r1, r2, rl = 0, 0, 0  \n",
    "    for a in batch_rouge_score:\n",
    "        r1 += a[0]\n",
    "        r2 += a[1]\n",
    "        rl += a[2]\n",
    "    total = len(batch_rouge_score)\n",
    "    if total == 0:\n",
    "        return epoch_loss / len(iterator), [0, 0, 0]\n",
    "    else:\n",
    "        return epoch_loss / len(iterator), [r1 / total,  r2 / total, rl / total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def load_record(path):\n",
    "    f = open(path, 'r')\n",
    "    losses = f.read()\n",
    "    losses = re.sub('\\\\]', '', losses)\n",
    "    losses = re.sub('\\\\[', '', losses)\n",
    "    losses = re.sub('\\\\,', '', losses)\n",
    "    losses = losses.split(' ')\n",
    "    losses = [float(i) for i in losses]\n",
    "    return losses, len(losses)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Dataset CNN/DailyMail </b>\n",
    "\n",
    "https://github.com/abisee/cnn-dailymail\n",
    "\n",
    "<code>\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['article', 'highlights', 'id'],\n",
    "        num_rows: <b>287113</b>\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['article', 'highlights', 'id'],\n",
    "        num_rows: <b>13368</b>\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['article', 'highlights', 'id'],\n",
    "        num_rows: <b>11490</b>\n",
    "    })\n",
    "})\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "3f672c405c5643fe8a0f0b86987bd147",
      "1cbdbe56454b43fa859b81668aa0f078",
      "44c63828b49d4de287b416de9ed0b74e",
      "4a5125a849f04089b79a68e29890baf5",
      "82a87fbbaff242bda20784252d2fb617",
      "fa76ab74cd404cf59c311100c5fa3dd0",
      "6c3a22f0364746a7aca31ce8ba135c8c",
      "aab75aa285b14458b9ecfcbc56c6c639",
      "2d4b6cb8ca6f4c168d08b012f26700bb",
      "121fc39ab956460ca0eddb3fe21f986a",
      "270b4150177a4a17980fa9fa5b9cc5c0",
      "730fd51fe7a9400c978aaf88fb199464",
      "26e2a46f41d14d8b8929639ebdd41ee2",
      "2e6ebf7b280e4d27b17d0e2531b77496",
      "957de83d75204f5b8f4f5c1676ec5ce7",
      "d3e955fb8af44b60b5cea7823718bcc7",
      "1e5eda7a99f94bbe84a557388218f229",
      "a3ef5eaeeed5481eb8622ae115fe8251",
      "310b36602bca40a1aa32ae4a9b33adb0",
      "c576c15ec2894de5b4cd02ba1f879824",
      "b0bc872c2cde423ca111d75dbed67d25",
      "51a507b111264916b726edd3a7c8ae37",
      "eb2a0c950833464ab314ef29edaa93c8",
      "f29ccaf465234a2cab38b99352dacaaf",
      "0dab73192efe4752b0bef1935f01baa1",
      "806ff3fe30d9431689fe126f92924a43",
      "2cb0cbba5a684bffbeabc67e4bc9113f",
      "a2013bb39c6448b6a0193db52dac4645",
      "d11692c237284f9bb25c9302f1f711a6",
      "3487ccbcd0454624b141116892a1ddd7",
      "5dc1546ddc3d455fab0d9cb31f65969f",
      "e2a0b4a56f304da6872785c1e13bb5a1",
      "c5f4cfcb3f1f420c936d9a50b3f93060",
      "cbbe90a82eb44ab999fec7e04531f708",
      "920b1a34650d411ca34d08558bde398b",
      "150a1aa18f8e455598f56ac513a92b5f",
      "684f74f349a641e5b7f768d100278939",
      "4ab47afd826649b8b8ae6fc810c2e6b0",
      "e97e9909065a4efea49d1d549038f82f",
      "f79419d341444a2bac7df44c7ff88540",
      "4c31bee902134848a682ebcc18443497",
      "f75bf428382e498d8b5abdf78fae03cb",
      "20d84523c394429daac023423ab774d6",
      "cd0e32eb00ee4242a97771e5a18be4c1",
      "37654833c21a45b7a04db86d1751d034",
      "1f960c11c72847bea0ace8f0e31802b0",
      "2cd7328d54b8411c8e77d3beb675eac7",
      "eacecdf986784167b1ac4f58eee9137c",
      "4939bc01d2c8412482ae89ce9f3dae6d",
      "5b0fd0beb6264417a2b57f1b7e4d1bf0",
      "8149cead6ed1454d9415526d25bebfda",
      "3129ce2c1bc842e7b04f01d88beb6995",
      "4178497a2df74fc198876c6c7834d518",
      "74151f27e2974795846153d8756302e1",
      "7b08e582814a4360af660bcdc9778a5d",
      "6a861396d9774c7399d5d5c01500b4f9",
      "bfb9c10017d949db9343b21172993bee",
      "2f819d35285e4fd39762f2bc93cc8b01",
      "aa23add07baf46f5af0722962bbbe2db",
      "0cbfca76613248489cb3134fe46fc70c",
      "6c729219ecdf449cb322ece1bf94b0e7",
      "aa072cd935dd4c2e98a1f6557d4e6282",
      "095a3beb16d04e6d8819387acf436a90",
      "0ee51456767849728f250d952b61b940",
      "feaeb575607049168c754cee2e90fd9d",
      "f0e02a5e0b834a61bcc916eed2361f3f",
      "85a2d182ec5f450cb52dcd41632d6477",
      "2f4ab7084d2d40dbb78a07172ca9fc25",
      "28b7529a03f444f0b7bb36c4abd20dfb",
      "819d53f4b4734f8a974af7023597efee",
      "0230d182265f4429bf0cc33fb84570aa",
      "d0b7888b191a4afab9b96f9b2deb343e",
      "1a6e832b761f45f688753e411f9f6b04",
      "7704d80c42484e34890dfb5eeb9715cb",
      "cfa868e7c06c4a00b95adc3c0ce10f18",
      "f0f9904b609345978c519e8aee57c584",
      "cb1fa5e590ef40b5b5d75efe3e827b60",
      "0bbbd03e3fd14dd780e8fa2f4fe4dafd",
      "fca297347b264c3cb3772e26f83c0ffe",
      "fa97e201b9da41b893ec142b322eb4a4",
      "61aa15b8aa314ae793720b4065025e5a",
      "7f86cfb7ac0444a7bf001b2f561fb62a",
      "b972a7f5326b4dc0ad939bb20b81101a",
      "6c8f17d29436408daafb2ed3fbce42de",
      "8a8927cc06864e3badccfa49a2180bf5",
      "c5045fa44cb94bbfb196390e2e2aac9f",
      "a63fabe501a245ea98f07a582a2d4e8d",
      "7cfe1175defa40269179b625afccdc93",
      "62afdb49e4904007a83f8f6178454a59",
      "2aee787e868f40e09d95e9a6a425ebdc",
      "ff385e9d14a243e692fc440da985bbcd",
      "61225b3c7c274485a8465c0e646c0620",
      "3bbf029aa96748c8b55e9567b68cfb03",
      "ba0a795a85fb4fd8b0baadfaea82c5d5",
      "4e180de1fd0247199fdbe3fbda429a49",
      "d9f8e6ff59a74b3ca62d8195c386081d",
      "d80e376e6429465ab38bd9f0a9eec82d",
      "5269474e826b4c299a243da716dbe088",
      "16a63205fc48492caecc1cc7d116f090",
      "fe2136e7d1f8477184399be21b9a9ad1",
      "4e42d2ebcf184d3cbc13bf8f2021d358",
      "4e77e690a33c4a61a984ddb11ab7f42f",
      "b59ec173c740495c82ecbaeb99fbaeea",
      "61f3e198516849b3bd0fc3a9888f33b0",
      "7aa4d6e6c97144b1a459a81659d7af51",
      "687917ed3651485293b840baa6106dbe",
      "dc891dda300d4e10a14ec03c9d98eb8a",
      "32561ea172a341caba8dc04df9b55a0c",
      "8c06d20e005143669051eb327e2132e9",
      "89e528a280d44d7e82bc05fbfdbc2e6f",
      "90e151bfcccb4dcc86d19d850dffcf29",
      "bb070264d17b4a6e8a76703b5b809880",
      "e3b90a800c6c471bb16c4c983cc7ff40",
      "306dd6e971ac4c249c659c6ac2e11c91",
      "97d5f8cbd8354bb79dec1ce9cd23b719",
      "71921208e256430f9b055ee01b428e87",
      "730bad801741425fb7839f18e86cef26",
      "6fba243c803d4d60bef8b4cd04dda500",
      "c2df1c756af24ee09fe58d8346567caf",
      "d64e209b78004e1582fcebd857e02e92",
      "d742913c8992494d95d7ef4d0d46f1ec",
      "f4da0f1fc08a41b5aa82db5f278de3da",
      "52af70afa5aa4fb5b7857ccb48c0c553",
      "5715f2f9ecdb4bae813d55f8c33daab0",
      "b8e8dedc505e419888368a874b77e0f6",
      "588f3659db8a4f7182f1c5cf82f45d49",
      "6f4437f3da8643da9052f8807360ca93",
      "aa8271c499b4419eb58a7f6ee0a58d00",
      "23a89917eaac4caa9ad9e5d17ffd2bd2",
      "7b61db16017c44f1b41929c0fa73d70d",
      "29abf8ddb6334b1c80a4f988df44b581",
      "d41fbc4f57b64b55add423372568d2de",
      "e5d4fa88efeb493db85c19a9dfb7dc91",
      "96cc8dfe84604969b718162192609c8d",
      "6a5cd5377a0e4fc29a447a18f2277829",
      "b9dcb81a8e664f7db2556972ff0824a7",
      "77efa48f7662446dabbc9b945f9136c1",
      "28330c389a7a422698f75ec73bdeb994",
      "7fa7b248ebb04193ae87fcdb90d3c058",
      "162a08ddc42c44aa856589b0a8e6afe7",
      "0fe62c643612485a9a0ad83110cf9393",
      "f562894920c247f18b06e24be4395f86",
      "03916a0622a54b58a12818114294cd77"
     ]
    },
    "id": "6LqlgstuncpO",
    "outputId": "aae0371d-7970-480e-8a1c-f29933f4f3b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c29cdafd32945af99e478214367776c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/azureuser/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3779b6418ae4b189740be2e598f0e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_ = load_dataset('cnn_dailymail', '3.0.0')\n",
    "dataset_2 = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model run </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 128\n",
    "max_len = 256\n",
    "max_sum_len = 50\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 1000\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(dataset_['train'])\n",
    "train_dataloader = DataLoader(dataset_['train'], sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validate_sampler = RandomSampler(dataset_['validation'])\n",
    "validate_dataloader = DataLoader(dataset_['validation'], sampler=validate_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer_train = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_pad_idx 1\n",
      "trg_pad_idx 1\n",
      "trg_sos_idx 0\n",
      "trg_eos_idx 2\n",
      "enc_voc_size 50265\n"
     ]
    }
   ],
   "source": [
    "trg_pad_idx = src_pad_idx = tokenizer_train.pad_token_id\n",
    "trg_sos_idx = tokenizer_train.bos_token_id\n",
    "trg_eos_idx = tokenizer_train.eos_token_id\n",
    "\n",
    "dec_voc_size = enc_voc_size = len(tokenizer_train)\n",
    "\n",
    "print('src_pad_idx %s' % src_pad_idx)\n",
    "print('trg_pad_idx %s' % trg_pad_idx)\n",
    "print('trg_sos_idx %s' % trg_sos_idx)\n",
    "print('trg_eos_idx %s' % trg_eos_idx)\n",
    "print('enc_voc_size %s' % enc_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load From Scratch </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 121,395,801 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27791/4208328862.py:22: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(50265, 512, padding_idx=1)\n",
       "      (pos_emb): PostionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(50265, 512, padding_idx=1)\n",
       "      (pos_emb): PostionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load from checkpoint</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 121,395,801 trainable parameters\n",
      "Test losses 921.1263728278024 train losses 924.6748081849433\n",
      "Resume from epoch 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses, train_count = load_record('./results/train_loss.txt')\n",
    "test_losses, _ = load_record('./results/test_loss.txt')\n",
    "rouges, _ = load_record('./results/rouges.txt')\n",
    "epoch -= train_count\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "print(f'Test losses {test_losses[len(test_losses) - 1]} train losses {train_losses[len(train_losses) - 1]}')\n",
    "print(f'Resume from epoch {epoch}')\n",
    "model.load_state_dict(torch.load(f\"./saved/model-{test_losses[len(test_losses) - 1]}.pt\"))\n",
    "#model.load_state_dict(torch.load(f\"Users/vladcioaba/transformers_exp/vanilla/saved/model-887.5220968201047.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "total_epoch = epoch\n",
    "best_loss = float('inf')\n",
    "\n",
    "train_iter = iter(train_dataloader)\n",
    "valid_iter = iter(validate_dataloader)\n",
    "\n",
    "train_losses, test_losses, rouges = [], [], []\n",
    "for step in range(total_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(tokenizer_train, model, train_iter, optimizer, criterion, clip, max_len)\n",
    "    valid_loss, rouge = evaluate(tokenizer_train, model, valid_iter, criterion, max_len)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if step > warmup:\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    rouges.append(rouge)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
    "\n",
    "    f = open('results/train_loss.txt', 'w')\n",
    "    f.write(str(train_losses))\n",
    "    f.close()\n",
    "\n",
    "    f = open('results/rouges.txt', 'w')\n",
    "    f.write(str(rouges))\n",
    "    f.close()\n",
    "\n",
    "    f = open('results/test_loss.txt', 'w')\n",
    "    f.write(str(test_losses))\n",
    "    f.close()\n",
    "\n",
    "    print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {np.exp(valid_loss):7.3f}')\n",
    "    print(f'\\tROUGE Score: {rouge}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Test eval </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 105/105 [06:27<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921.1263728278024 [0.06861149342184318, 0.002915726042846819, 0.06816946234603505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_iter = iter(validate_dataloader)\n",
    "valid_loss, rouge = evaluate(tokenizer_train, model, valid_iter, criterion, max_len)\n",
    "print(valid_loss, rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create random sample for validation  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "test_sampler = RandomSampler(dataset_['test'])\n",
    "test_dataloader = DataLoader(dataset_['test'], sampler=test_sampler, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xm0mb8OPdJ4r",
    "outputId": "11065397-7489-4840-840f-3f4f37a86939"
   },
   "source": [
    "<b> Comparison with pretrained bart-large-cnn model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0acb33d74e0d46eea781250e269a2b15",
      "956316942c6143d696e5e40961a44ebf",
      "1b1a802eba8d48a690ddf7b60621df38",
      "1c3e28f4e8a34f4187b3ed2962307450",
      "5a405021fd9f4d218849d8b95f28a848",
      "6b888c630c174b679cf24d032f3868ff",
      "ad6afd5354024cd19c82af5255cf8f02",
      "a3848e43d67c46138d9157ab32d16fc6",
      "d2af93d63574441489aa0186ffb7c95f",
      "959a6906711244ab9b44f900f29e0035",
      "928526362ce04612a2a379383e862fd9"
     ]
    },
    "id": "vQtvWMhZ1SvW",
    "outputId": "553ce993-213f-4cec-a24f-450bb5f76658"
   },
   "outputs": [],
   "source": [
    "# load model from higgingface\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "tokenizer_test = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model_test = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "7WddSlh1GA0c",
    "outputId": "e019feda-64e1-4487-8563-43323dfa6108"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "100%|| 90/90 [3:54:41<00:00, 156.46s/it]  \n",
      "100%|| 11490/11490 [00:00<00:00, 476107.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "rouge-1: r:0.41, p:0.17 f:0.41\n",
      "rouge-2: r:0.28, p:0.11 f:0.28\n",
      "rouge-l: r:0.61, p:0.35 f:0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model and print rouge score\n",
    "\n",
    "articles_list = []\n",
    "highlights_list = []\n",
    "generated_list = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        sample_list = list(zip(batch['article'], batch['highlights']))\n",
    "        for article, highlights in sample_list:\n",
    "            text = article.strip().replace(\"\\n\",\"\")\n",
    "            hightlights = highlights.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "            input_ids = tokenizer_test.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "            outputs = model_test.generate(input_ids, num_beams=4, no_repeat_ngram_size=2, early_stopping=True)\n",
    "            generated_list.append(tokenizer_test.decode(outputs[0]))\n",
    "            articles_list.append(text)\n",
    "            highlights_list.append(highlights)\n",
    "\n",
    "print_rouge_score(generated_list, highlights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(src, model, tokenizer, device, max_sum_len):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    src_tensor = tokenizer.encode(src, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n",
    "    src_mask = (src_tensor != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "        \n",
    "    trg_indexes = [1]\n",
    "\n",
    "    for i in range(max_sum_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_pad_mask = (trg_tensor != trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg_tensor.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_eos_idx :\n",
    "            break\n",
    "    \n",
    "    trg_tokens = tokenizer_test.decode(trg_indexes)\n",
    "    return trg_tokens[7:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN / DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 128/128 [01:11<00:00,  1.78it/s]\n",
      "100%|| 128/128 [00:00<00:00, 388473.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "rouge-1: r:0.21, p:0.00 f:0.21\n",
      "rouge-2: r:0.37, p:0.00 f:0.37\n",
      "rouge-l: r:0.38, p:0.00 f:0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated2 = []\n",
    "highlights2 = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        sample_list = list(zip(batch['article'], batch['highlights']))\n",
    "        for article, highlights in sample_list:\n",
    "            outputs = predict(article, model, tokenizer_train, device, 50)\n",
    "            generated2.append(outputs)\n",
    "            highlights2.append(highlights)\n",
    "\n",
    "print_rouge_score(generated2, highlights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler2 = RandomSampler(dataset_2['test'])\n",
    "test_dataloader2 = DataLoader(dataset_2['test'], sampler=test_sampler2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 89/89 [1:43:37<00:00, 69.85s/it]\n",
      "100%|| 11334/11334 [00:00<00:00, 34858.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "rouge-1: r:0.00, p:0.00 f:0.00\n",
      "rouge-2: r:0.21, p:0.00 f:0.21\n",
      "rouge-l: r:0.00, p:0.00 f:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated3 = []\n",
    "highlights3 = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(test_dataloader2), total=len(test_dataloader2)):\n",
    "        sample_list = list(zip(batch['document'], batch['summary']))\n",
    "        for article, highlight in sample_list:\n",
    "            outputs = predict(article, model, tokenizer_train, device, 50)\n",
    "            generated3.append(outputs)\n",
    "            highlights3.append(highlight)\n",
    "\n",
    "print_rouge_score(generated3, highlights3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Looking after someone with dementia can stretch people to their limits, and there are many in this situation.\\xa0In England alone, there are more than 670,000 unpaid carers helping someone with dementia. Here, in the final week of our major Good Health series on dementia, we turn our attention to the carers and what can be done to make life easier for them and their loved ones... Scroll down for video . Looking after someone with dementia can stretch people to their limits. Here's how to\\xa0make life easier . WHAT TO EXPECT . The early stages of the disease bring changes that may be so subtle that some friends and acquaintances are unaware that there is anything wrong - and this stage can continue for many years. But as the disease progresses and more damage is done to the brain, symptoms become more pronounced: difficulties with communication become more intense and issues such as getting dressed or managing day-to-day affairs become more problematic. This marks the beginning of the middle stage, the longest stage, which can last for several years. It's when the condition becomes more challenging and extra help may be needed. However, there will be good days as well as bad. The following can help . . . ESTABLISH ROUTINES . In the early stages of dementia, it's the short-term memory that's most affected, making it harder for patients to keep track of the day, or even what time of day it is. I've found that sticking to a routine helps keep someone with dementia better orientated, and gets their body clock into a rhythm. In the early stages, aim to establish fixed times of day for the following - it may help to write this on a board that's kept next to a calendar (cross it off to show what day it is) and a clock, so that they know what to expect when. In the middle stages of dementia, routines may still be useful but patients can be more prone to mood changes, so be flexible. Encourage but don't coerce. Often people with dementia may display more symptoms and anxiety as the light starts to fade . WHEN BEHAVIOUR CHANGES . The middle stage of dementia inevitably heralds changes in behaviour. This can be one of the toughest things to cope with, for at times it may seem as if the person you know and love has gone. The following are common: . Clinginess . This can be very draining as some people with dementia do not like to let their carer out of their sight. Clinginess is the return of a psychological reflex from our toddler years - a toddler will cry when their mother moves away or when a stranger approaches. As the dementia progresses, the loss of nerve cells in the brain allows old reflexes that were replaced in adulthood by more complex brain pathways to resurface. What to do: The best way to deal with it is by diluting contact - try introducing other regular carers as soon as the person being cared for starts to display any clingy behaviour. The clinging behaviour may now apply to many people, not just one. Wandering . Around 60 per cent of those with dementia wander - pacing around the house or outside of the home - and the problem is they can get lost, as the damage to the hippocampus part of the brain means they lose their sense of orientation. What to do: If they tell you they're going out don't argue but speak calmly and tell them they don't need to as they are 'staying here' tonight. Avoid busy places such as shopping centres. People with dementia may find these disorientating and when they get home this disorientation can continue and induce wandering so they may feel the urge to leave the house. Wandering about the house at night can arise from a need to go to the loo. Ensure they don't have too much fluid before bed and install night-lights to help prevent falls. Painting the walls and doors a matching colour makes it less likely that the person with dementia will find their way out. You could also consider moving the lock higher up the door to a less familiar place. You may want to consider installing movement sensors to alert you to potential problems. Anxiety . Worrying for no reason or displaying physical symptoms such as palpitations are signs that the dementia patient may be suffering with anxiety. What to do: Try to identify what's causing it - have there been more people round than normal, or have they had too much caffeine? Many of the triggers may be unavoidable, but it can help to keep their surroundings as comfortable and relaxing as possible. Put pictures and familiar possessions around them. Pleasant smells such as flowers, baking aromas and soap may trigger memories that help distract them from their anxious feelings. Playing music can also be calming. Talking to them can distract them, too. But never talk down to them or use childish language as this could make them angry. Anger and aggression . It can be confusing when someone who was once so patient and tolerant becomes prone to temper tantrums. What to do: It's important to consider any physical reasons that might be making them this way. Pain is a common cause of anger in those with dementia, as are urinary or chest infections, a stroke, or even something as simple as not having had enough sleep. If you suspect any of these is possible, then speak to a doctor. Speak to the person with dementia reassuringly, in a calm manner. Use music or a calming activity such as massaging or stroking their hand or brushing their hair to distract them. If, however, they seem aggressive do not put yourself in danger. If necessary, move back and wait for them to calm down. Keep the person busy at sundown and in the early evening . Sundowning . Often people with dementia have good and bad days but also good and bad times during a given day - and often they may display more symptoms and anxiety as the light starts to fade, becoming confused, agitated, restless or showing repetitive behaviours. This pattern is referred to as 'sundowning'. It's not clear why this happens - it's possible it's to do with a disruption to brain chemicals affecting the body clock. Two-thirds of those with Alzheimer's disease experience sundowning. It can occur at any stage of the condition, but tends to peak at the middle stage and lessen as the disease progresses. What to do: Keep the person busy at sundown and in the early evening. Avoid stimulants such as coffee or alcohol at this time of day as this may make them more excitable. A rocking chair can help as it is soothing and relaxing. It may also help to introduce more light into the room. LOOK THEM IN THE EYE WHEN YOU TALK . With dementia, the parts of the brain that normally deal with understanding, thought processing and language deteriorate. The patient may struggle with nuances and may lose their sense of humour; they may also get frustrated and cross when they can't quickly bring to mind the words they need. Furthermore, they may seem to get stuck in a loop and repeat the same question again and again. The following strategies can help tackle these issues: . Better to say: 'You seem to have put the crockery in the washing machine. Did you mean to put them in the dishwasher?'  Dr Souter is a retired GP and fellow of the Royal College of General Practitioners. Adapted by LUCY ELKINS from Your Guide To Understanding And Dealing With Dementia by Dr Keith Souter, published by Summersdale, 8.99. Order at www.mailbookshop.co.uk, or call 0808 272 0808, p&p is free for a limited time only.\", \"This is the\\xa0final week of our major Good Health series on dementia .\\nWe turn our attention to carers and what can be done to make life easier .\\nIn England there are more than 670,000 unpaid carers helping someone .\\nWe explain what to if they become anxious as the light starts to fade .\\nBed and getting-up times.\\nWhen to take medication.\\nMeal times.\\nShopping days.\\nLeisure time such as TV, radio or social times.\\nBefore you start talking always engage eye contact to help get the person with dementia to focus on you. There can be a tendency for them to look away and this makes it harder for them to concentrate on the conversation.\\nBe careful about asking questions they may be unable to process. For example, if they're upset don't ask what's wrong as they may not know, or are unable to put it into words. Instead, tell them: 'You seem to be upset; let's think how we can make you feel better.'\\nFinishing sentences for them can increase frustration. It's better to ask if they'd like you to find words for them if they're having difficulty.\\nWatch their body language as this can give you clues about how they are feeling. For example, repetitive movements can mean they are anxious or scared, while withdrawing may mean they feel overwhelmed.\\nIf they're doing something obviously wrong, for example putting dishes in the washing machine or clothes in a food cupboard, don't ask them why or castigate them - the reasoning side of their brain has been affected and pointing out their mistakes will only cause them embarrassment and frustration.\")\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "m = 10000\n",
    "\n",
    "for idx, a in enumerate(sample_list):\n",
    "    if len(a) < m:\n",
    "        m = len(a)\n",
    "        idx = idx\n",
    "print(sample_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda Bethesda'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"\"\"For example, repetitive movements can mean they are anxious or scared, while \n",
    "withdrawing may mean they feel overwhelmed.\\nIf they're doing something obviously wrong, \n",
    "for example putting dishes in the washing machine or clothes in a food cupboard, don't ask\n",
    "them why or castigate them - the reasoning side of their brain has been affected and pointing \n",
    "out their mistakes will only cause them embarrassment and frustration.\"\"\", model, tokenizer_train, device, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0853dcd594054f418a96354c6f01adee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0f3b38aafaa84a838922c93109a5c44b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f7911e75e6d4b828cab51e3195b8436": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "14172bb678994f2dbb94b695a3b55b8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5c08976aa10e45bc9753454e6ebbc802",
       "style": "IPY_MODEL_2cc853e413fc4829bf282e581ce34ab5",
       "value": "100%"
      }
     },
     "1f335a2f0aab43e3ab2be83cc92d0ff6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9585d691db924147b95107d47272adf0",
       "max": 3,
       "style": "IPY_MODEL_ea4baec3e4dd4385a00dc7c066c61d59",
       "value": 3
      }
     },
     "2c29cdafd32945af99e478214367776c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_14172bb678994f2dbb94b695a3b55b8f",
        "IPY_MODEL_1f335a2f0aab43e3ab2be83cc92d0ff6",
        "IPY_MODEL_9e9be4a164aa48299fc5c42bc6585501"
       ],
       "layout": "IPY_MODEL_9794294d2b604667b6b40b6ddf0e703a"
      }
     },
     "2cc853e413fc4829bf282e581ce34ab5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5c08976aa10e45bc9753454e6ebbc802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68935bbdf70940a08f2d38c5fc4813b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_693a9ad3d5454f628b7f3f60308f01fe",
       "max": 3,
       "style": "IPY_MODEL_0853dcd594054f418a96354c6f01adee",
       "value": 3
      }
     },
     "693a9ad3d5454f628b7f3f60308f01fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b7fcc95caf64f04b0da83565a19c25d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "812703f8b9b14cabbc70d490c977354b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "81e56075e3404ad2a13d8e0601707cb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0f3b38aafaa84a838922c93109a5c44b",
       "style": "IPY_MODEL_0f7911e75e6d4b828cab51e3195b8436",
       "value": "100%"
      }
     },
     "9585d691db924147b95107d47272adf0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9794294d2b604667b6b40b6ddf0e703a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e9be4a164aa48299fc5c42bc6585501": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7b7fcc95caf64f04b0da83565a19c25d",
       "style": "IPY_MODEL_cd8841eccc95454f883450e2a0372cdc",
       "value": " 3/3 [00:00&lt;00:00, 120.46it/s]"
      }
     },
     "9f613abb7b754a23a053e5d41f21cb56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b11eab01693c4c4891dfebda80270cd1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cd8841eccc95454f883450e2a0372cdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d64b910f5303455d96be07e4d3800304": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9f613abb7b754a23a053e5d41f21cb56",
       "style": "IPY_MODEL_812703f8b9b14cabbc70d490c977354b",
       "value": " 3/3 [00:00&lt;00:00, 125.72it/s]"
      }
     },
     "e3779b6418ae4b189740be2e598f0e6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_81e56075e3404ad2a13d8e0601707cb7",
        "IPY_MODEL_68935bbdf70940a08f2d38c5fc4813b2",
        "IPY_MODEL_d64b910f5303455d96be07e4d3800304"
       ],
       "layout": "IPY_MODEL_b11eab01693c4c4891dfebda80270cd1"
      }
     },
     "ea4baec3e4dd4385a00dc7c066c61d59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
