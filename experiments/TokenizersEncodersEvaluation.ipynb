{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b3a69b-f8a2-468c-a9fa-fc488ae8c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_text_1 = '''Sodium hypochlorite (commonly known in a dilute solution as bleach) is an inorganic chemical compound with the formula NaOCl (or NaClO), \n",
    "                      comprising a sodium cation (Na+) and a hypochlorite anion (OCl− or ClO−). '''\n",
    "\n",
    "#interesting_text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9896a879-a66a-4f8e-b119-0337916bddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ebf3f-727d-4fb3-888b-20f4ae033b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, EncoderDecoderModel\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer_bart = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model_bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model_xlnet = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model_roberta = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "tokenizer_electra = ElectraTokenizer.from_pretrained(\"google/electra-base-generator\")\n",
    "model_electra = AutoModel.from_pretrained(\"google/electra-base-generator\")\n",
    "\n",
    "\n",
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model_distilbert = AutoModel.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "#tokenizer_codebert = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "#model_codebert = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbe543e-0baa-4a54-a4d4-c57c028d015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('shared', Embedding(50264, 1024, padding_idx=1)),\n",
       "              ('encoder',\n",
       "               BartEncoder(\n",
       "                 (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "                 (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "                 (layers): ModuleList(\n",
       "                   (0): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (1): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (2): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (3): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (4): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (5): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (6): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (7): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (8): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (9): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (10): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (11): BartEncoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               )),\n",
       "              ('decoder',\n",
       "               BartDecoder(\n",
       "                 (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "                 (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "                 (layers): ModuleList(\n",
       "                   (0): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (1): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (2): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (3): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (4): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (5): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (6): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (7): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (8): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (9): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (10): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                   (11): BartDecoderLayer(\n",
       "                     (self_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (encoder_attn): BartAttention(\n",
       "                       (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                     (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               ))]),\n",
       " 'config': BartConfig {\n",
       "   \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
       "   \"_num_labels\": 3,\n",
       "   \"activation_dropout\": 0.0,\n",
       "   \"activation_function\": \"gelu\",\n",
       "   \"add_final_layer_norm\": false,\n",
       "   \"architectures\": [\n",
       "     \"BartForConditionalGeneration\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classif_dropout\": 0.0,\n",
       "   \"classifier_dropout\": 0.0,\n",
       "   \"d_model\": 1024,\n",
       "   \"decoder_attention_heads\": 16,\n",
       "   \"decoder_ffn_dim\": 4096,\n",
       "   \"decoder_layerdrop\": 0.0,\n",
       "   \"decoder_layers\": 12,\n",
       "   \"decoder_start_token_id\": 2,\n",
       "   \"dropout\": 0.1,\n",
       "   \"early_stopping\": true,\n",
       "   \"encoder_attention_heads\": 16,\n",
       "   \"encoder_ffn_dim\": 4096,\n",
       "   \"encoder_layerdrop\": 0.0,\n",
       "   \"encoder_layers\": 12,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"force_bos_token_to_be_generated\": true,\n",
       "   \"forced_bos_token_id\": 0,\n",
       "   \"forced_eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"id2label\": {\n",
       "     \"0\": \"LABEL_0\",\n",
       "     \"1\": \"LABEL_1\",\n",
       "     \"2\": \"LABEL_2\"\n",
       "   },\n",
       "   \"init_std\": 0.02,\n",
       "   \"is_encoder_decoder\": true,\n",
       "   \"label2id\": {\n",
       "     \"LABEL_0\": 0,\n",
       "     \"LABEL_1\": 1,\n",
       "     \"LABEL_2\": 2\n",
       "   },\n",
       "   \"length_penalty\": 2.0,\n",
       "   \"max_length\": 142,\n",
       "   \"max_position_embeddings\": 1024,\n",
       "   \"min_length\": 56,\n",
       "   \"model_type\": \"bart\",\n",
       "   \"no_repeat_ngram_size\": 3,\n",
       "   \"normalize_before\": false,\n",
       "   \"num_beams\": 4,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"output_past\": true,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"prefix\": \" \",\n",
       "   \"scale_embedding\": false,\n",
       "   \"task_specific_params\": {\n",
       "     \"summarization\": {\n",
       "       \"early_stopping\": true,\n",
       "       \"length_penalty\": 2.0,\n",
       "       \"max_length\": 142,\n",
       "       \"min_length\": 56,\n",
       "       \"no_repeat_ngram_size\": 3,\n",
       "       \"num_beams\": 4\n",
       "     }\n",
       "   },\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50264\n",
       " },\n",
       " 'name_or_path': 'facebook/bart-large-cnn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73936496-f50a-44de-b6e4-323c61a576d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('embeddings',\n",
       "               BertEmbeddings(\n",
       "                 (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "                 (position_embeddings): Embedding(512, 768)\n",
       "                 (token_type_embeddings): Embedding(2, 768)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('encoder',\n",
       "               BertEncoder(\n",
       "                 (layer): ModuleList(\n",
       "                   (0): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (2): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (3): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (4): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (5): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (6): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (7): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (8): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (9): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (10): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (11): BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )),\n",
       "              ('pooler',\n",
       "               BertPooler(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (activation): Tanh()\n",
       "               ))]),\n",
       " 'config': BertConfig {\n",
       "   \"_name_or_path\": \"bert-base-cased\",\n",
       "   \"architectures\": [\n",
       "     \"BertForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 512,\n",
       "   \"model_type\": \"bert\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"type_vocab_size\": 2,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 28996\n",
       " },\n",
       " 'name_or_path': 'bert-base-cased'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e8cb2e-4c66-4085-886a-da08324725c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict([('mask_emb',\n",
       "               Parameter containing:\n",
       "               tensor([[[-3.8489e-03,  5.2486e-03, -6.3917e-03, -1.8904e-03,  5.0990e-03,\n",
       "                         -5.7584e-03, -7.6392e-03,  8.1679e-03,  4.7458e-03, -4.4163e-03,\n",
       "                         -6.7998e-03,  9.4918e-03, -9.9556e-04, -7.6653e-03, -2.3268e-03,\n",
       "                          1.3749e-04,  1.5165e-02, -5.7319e-03,  2.6785e-03,  6.4740e-03,\n",
       "                          6.3571e-03, -6.1715e-03, -7.1400e-03, -3.8910e-02,  1.7696e-02,\n",
       "                          9.2599e-03,  2.5634e-03,  4.6978e-03,  5.6817e-03, -9.1121e-04,\n",
       "                          7.7846e-04,  8.9368e-03, -9.4092e-04, -7.0046e-03,  2.4308e-03,\n",
       "                         -8.3177e-03,  1.7004e-03, -5.0066e-03, -3.9071e-03,  3.8386e-03,\n",
       "                          2.1928e-02, -3.3668e-03,  5.0940e-03, -4.7822e-03, -1.4946e-02,\n",
       "                         -4.6191e-03, -1.0626e-03,  4.5942e-04, -8.2133e-03,  7.4550e-04,\n",
       "                          6.4538e-03, -1.4207e-02,  5.2352e-04,  1.8810e-03,  2.1082e-03,\n",
       "                         -3.1629e-03,  5.7984e-03, -9.1619e-03, -1.6809e-03, -7.7128e-04,\n",
       "                          6.3614e-03,  1.4638e-02, -1.0036e-02,  5.3371e-03, -1.3999e-02,\n",
       "                         -2.6316e-03,  3.5948e-03, -3.6184e-03, -1.0520e-02,  6.9548e-03,\n",
       "                         -3.9991e-03, -1.6961e-02,  9.1949e-03, -1.2533e-02, -8.2309e-03,\n",
       "                          5.1060e-03, -3.5367e-03,  1.5067e-02, -2.3074e-02, -3.5849e-03,\n",
       "                         -6.6330e-03,  1.1239e-02,  5.6042e-03, -2.2431e-03, -7.4257e-03,\n",
       "                         -6.1267e-03,  1.1370e-02,  1.1899e-02,  3.8417e-03,  1.4847e-03,\n",
       "                          4.6581e-03, -5.7979e-04, -7.1888e-03, -7.0490e-03, -5.4599e-03,\n",
       "                          1.5454e-03, -7.4947e-03,  3.4394e-03,  1.2842e-03,  1.7190e-02,\n",
       "                         -3.1542e-03, -4.3306e-03,  1.0499e-02, -1.6634e-02,  8.8821e-03,\n",
       "                         -6.1461e-03, -1.1100e-02,  5.5859e-03, -1.2589e-02, -3.7537e-03,\n",
       "                          8.5574e-03, -1.5335e-02, -8.5612e-03,  5.3078e-03, -8.4830e-03,\n",
       "                         -3.0820e-03,  6.4509e-04,  7.4449e-03,  2.5576e-03,  2.2456e-03,\n",
       "                          2.4755e-03,  5.0237e-03, -7.3106e-03,  1.3923e-02,  4.5775e-03,\n",
       "                          8.2297e-04,  1.0839e-02, -4.4726e-03,  2.5474e-03,  5.4113e-03,\n",
       "                         -1.2617e-03,  1.1615e-02,  8.0871e-03,  3.3243e-03,  5.3191e-03,\n",
       "                         -6.0738e-03,  6.8991e-05,  3.5439e-03, -4.2568e-03, -8.4728e-03,\n",
       "                          5.2417e-03,  1.5981e-03, -1.3135e-02, -1.5808e-02,  4.0288e-03,\n",
       "                         -9.2005e-04,  2.5101e-03, -6.9480e-03, -2.2872e-03,  4.4352e-04,\n",
       "                          4.5829e-03,  7.0086e-03, -4.3485e-03, -6.1437e-03,  7.9139e-04,\n",
       "                          8.3002e-03, -6.4215e-03,  2.4735e-03, -6.3016e-04,  7.4905e-03,\n",
       "                          1.6721e-02,  2.4368e-03,  7.3777e-03, -1.7766e-02, -5.5397e-03,\n",
       "                          6.9294e-03, -6.6666e-03,  2.8042e-03, -6.6126e-04, -6.5705e-04,\n",
       "                          2.5191e-03,  6.6034e-03,  3.7766e-03, -8.4218e-03,  6.5971e-03,\n",
       "                         -2.3056e-03, -5.4652e-03, -5.2349e-03, -4.3385e-03,  5.1741e-04,\n",
       "                          8.5096e-03, -3.0605e-03,  2.1552e-02,  4.5428e-03, -1.6986e-05,\n",
       "                         -7.5176e-03, -7.7604e-03, -8.0304e-03,  2.9035e-03, -2.1807e-03,\n",
       "                          2.9130e-03, -2.1851e-03, -5.1454e-03, -2.0150e-02,  1.1022e-02,\n",
       "                          6.6065e-03,  5.6859e-03,  3.9572e-03,  6.5844e-03, -9.0039e-03,\n",
       "                         -1.1197e-02,  2.3821e-03, -5.7874e-03,  1.6338e-02, -1.6098e-03,\n",
       "                         -1.0034e-02,  2.7962e-03, -6.0697e-03, -1.1963e-02, -2.2777e-04,\n",
       "                         -1.2146e-02, -2.8212e-01, -9.6147e-03,  1.5929e-03, -1.1507e-03,\n",
       "                         -1.9201e-03,  1.4730e-02, -2.2899e-03,  5.1997e-03, -4.1450e-03,\n",
       "                          5.9884e-03,  6.2505e-03, -1.7106e-03,  1.4841e-03, -6.8338e-03,\n",
       "                         -5.8864e-03, -1.5350e-02,  1.0628e-03, -2.2193e-03,  1.0743e-02,\n",
       "                          1.9733e-02, -6.9488e-03,  2.2586e-03,  1.5780e-03,  1.1838e-03,\n",
       "                         -5.0491e-03,  3.8282e-03,  5.0812e-03, -2.0774e-02,  1.0427e-02,\n",
       "                          1.1380e-02,  7.4795e-03, -7.2652e-03,  7.2139e-03,  7.2564e-03,\n",
       "                          1.0699e-02,  3.1426e-03,  8.1635e-03, -8.6605e-03, -9.5233e-04,\n",
       "                         -3.4093e-03,  4.0424e-03, -8.2425e-04, -1.1444e-03, -1.6169e-02,\n",
       "                          6.4705e-04,  6.3446e-02,  2.5057e-02, -1.2801e-03, -4.9292e-03,\n",
       "                         -3.8479e-03, -9.3692e-03,  2.6148e-03, -2.5583e-03, -6.5228e-03,\n",
       "                         -1.4817e-04, -1.2636e-02,  1.3982e-04,  2.5264e-03,  2.4116e-03,\n",
       "                          7.3369e-03, -8.9302e-03,  5.1205e-03, -4.7891e-03,  4.8993e-03,\n",
       "                          8.0395e-03,  5.1685e-03,  7.6004e-03,  2.3221e-03,  1.7775e-02,\n",
       "                         -3.5043e-03, -5.9930e-03, -6.1874e-03,  1.5804e-03, -4.0868e-03,\n",
       "                         -2.9228e-03, -8.7827e-04,  8.1440e-03, -4.2319e-03,  2.6551e-04,\n",
       "                         -1.0752e-03, -8.4520e-04, -3.7217e-03, -3.9043e-03, -1.0686e-02,\n",
       "                         -6.8234e-03, -2.3705e-03,  7.6363e-03,  1.5767e-02, -3.5364e-03,\n",
       "                          4.9046e-03,  1.7796e-03, -4.5567e-03, -4.3101e-04, -2.0601e-03,\n",
       "                          7.7408e-03, -3.8118e-03,  2.8563e-03, -5.5439e-04, -6.7817e-03,\n",
       "                         -3.7755e-03, -9.4172e-03, -1.5801e-02,  3.4868e-03,  1.0480e-02,\n",
       "                          7.8893e-03,  2.3728e-02, -2.9015e-03, -1.7074e-02, -4.9551e-04,\n",
       "                          1.4089e-03, -9.1218e-03,  1.3239e-03, -2.2413e-03,  9.9259e-03,\n",
       "                         -2.5574e-03, -4.4924e-03,  1.4720e-03, -5.8499e-04, -6.8539e-03,\n",
       "                         -3.4361e-01,  7.1393e-03,  1.7698e-03, -1.2381e-02,  4.0259e-03,\n",
       "                         -1.9451e-03,  3.4497e-03, -1.0168e-02,  2.7775e-03,  5.7782e-03,\n",
       "                          1.0874e-03, -7.5929e-04, -1.0232e-03, -7.0317e-03, -3.8674e-03,\n",
       "                          9.4150e-03,  5.2106e-03, -9.6721e-03,  4.6426e-03, -1.2157e-02,\n",
       "                          8.6827e-04, -2.4965e-03, -3.0448e-02,  1.5991e-02,  7.6731e-03,\n",
       "                         -2.6503e-03, -5.1221e-03, -9.2250e-04, -5.6297e-04, -6.3397e-03,\n",
       "                          6.8051e-03,  1.1757e-03, -6.5315e-03, -2.1275e-03, -3.1032e-03,\n",
       "                         -4.7134e-04, -4.9693e-03, -3.7160e-03,  5.3980e-03,  2.6673e-04,\n",
       "                         -1.1918e-02, -4.2398e-03, -2.4492e-04,  2.9423e-04, -2.3079e-03,\n",
       "                          2.1739e-03, -1.0474e-02, -2.8692e-03,  9.3944e-03,  2.3155e-03,\n",
       "                          4.2974e-04, -6.0029e-03,  1.2447e-02,  8.6333e-04,  3.5206e-02,\n",
       "                          7.5244e-03, -7.4279e-03,  1.5742e-02, -1.0204e-04, -5.1424e-04,\n",
       "                          9.8140e-01,  9.4751e-03, -1.0505e-02,  4.4565e-03,  3.0387e-03,\n",
       "                          9.7431e-03, -6.6461e-03,  5.8948e-04, -3.2662e-03,  4.0614e-03,\n",
       "                          8.4468e-03,  6.3052e-03, -2.4076e-03,  3.5225e-03,  5.4984e-03,\n",
       "                          6.0468e-03,  7.0341e-04, -3.8260e-03, -1.3227e-03,  7.3870e-05,\n",
       "                          2.5830e-03, -3.6014e-03, -1.9832e-02, -5.9054e-03, -1.0409e-02,\n",
       "                          1.2352e-02,  1.1229e-02,  1.2017e-02,  8.3185e-03,  2.0455e-03,\n",
       "                          7.2846e-03, -8.7301e-03, -5.0110e-03,  8.6028e-03, -7.8373e-03,\n",
       "                          8.0234e-03, -3.7387e-03,  1.2014e-02,  3.8757e-03, -4.9016e-03,\n",
       "                          7.9857e-03, -1.2795e-02, -9.8447e-03, -1.6537e-02, -1.2619e-03,\n",
       "                         -2.4265e-03,  3.8493e-03,  1.4722e-03,  1.5786e-03, -2.4075e-03,\n",
       "                         -1.2961e-02, -9.2017e-05, -1.7226e-03, -4.5209e-03,  9.5085e-03,\n",
       "                          1.1711e-02, -9.5369e-04, -2.5680e-03,  8.2729e-04,  6.7849e-03,\n",
       "                          3.5676e-03, -1.7611e-03, -7.4642e-04,  5.8282e-03,  1.1570e-03,\n",
       "                          1.1034e-02, -2.3434e-03,  1.4307e-03, -6.5965e-03,  8.2939e-03,\n",
       "                         -9.5055e-04,  1.6509e-02,  4.9875e-03,  5.5945e-03, -7.0535e-03,\n",
       "                         -4.0763e-04, -2.4045e-03,  1.0367e-02, -3.8753e-03, -1.3131e-02,\n",
       "                          1.4685e-02,  8.2025e-03,  5.8760e-03, -1.5105e-02, -6.9171e-03,\n",
       "                         -2.8907e-03, -1.5368e-02,  6.2403e-03, -4.6092e-03,  3.1971e-04,\n",
       "                          1.9281e-03,  4.7095e-03,  9.6845e-04, -2.3760e-03, -8.6279e-03,\n",
       "                          9.9484e-04, -5.1237e-03, -1.5471e-02, -6.5875e-04,  6.2680e-03,\n",
       "                         -2.9289e-03, -8.4267e-03, -4.0129e-03, -1.3937e-03, -5.0108e-05,\n",
       "                         -1.1317e-02, -1.6491e-03, -4.9891e-03,  1.5530e-03,  1.8759e-04,\n",
       "                          3.7405e-03,  4.8540e-04,  4.0490e-04, -5.5665e-03,  8.9043e-03,\n",
       "                         -1.1102e-03, -4.5872e-03,  9.6644e-03,  4.2239e-04, -1.4963e-03,\n",
       "                         -1.4578e-03,  5.7079e-03,  7.0796e-03, -9.7693e-03, -3.4695e-04,\n",
       "                          3.2159e-03, -4.5082e-03,  5.5317e-03, -6.9175e-03,  1.5951e-02,\n",
       "                          8.2272e-03,  8.6929e-03, -1.4809e-02, -1.4144e-02,  8.7115e-03,\n",
       "                         -4.5788e-03,  9.3471e-03,  2.7539e-03,  1.8595e-03,  9.7944e-03,\n",
       "                         -1.4078e-03,  1.9463e-03,  1.0584e-02,  5.6703e-03, -1.5885e-02,\n",
       "                          2.5174e-03,  6.2044e-03,  2.0145e-03, -1.5673e-03, -8.0917e-03,\n",
       "                          5.1342e-03,  1.9486e-03,  7.8459e-03, -7.5107e-03,  2.1192e-03,\n",
       "                          5.3546e-03, -8.1919e-04, -1.3524e-02, -2.5200e-03, -1.3210e-02,\n",
       "                         -5.3595e-03,  5.3429e-03, -3.2909e-03,  2.6990e-02, -6.9699e-03,\n",
       "                         -5.0182e-03, -1.7021e-02, -4.0467e-04,  1.4194e-04, -7.4493e-03,\n",
       "                         -8.7065e-03, -8.0794e-03, -3.8297e-03,  4.5074e-03, -2.1655e-03,\n",
       "                         -3.3375e-03,  4.0825e-03, -6.6070e-03, -2.9961e-03,  9.2062e-03,\n",
       "                         -6.0483e-03, -1.3044e-03,  1.8094e-03, -5.0790e-03,  5.7293e-03,\n",
       "                         -1.7150e-04, -5.5665e-03, -7.9151e-03, -2.8639e-03,  1.9791e-04,\n",
       "                          4.8750e-03, -1.3828e-02, -8.4893e-03, -3.8919e-03,  2.7241e-03,\n",
       "                         -1.2843e-02,  7.0930e-03,  5.7145e-03, -1.4913e-03,  2.7551e-02,\n",
       "                          3.5249e-03,  2.8517e-04,  6.1544e-03,  2.9267e-03,  2.5690e-03,\n",
       "                         -5.3984e-03, -8.7254e-03,  3.9059e-03, -2.2225e-03,  5.0395e-03,\n",
       "                         -8.5649e-04,  9.3596e-03,  8.7518e-03,  2.1451e-03,  3.3005e-03,\n",
       "                          2.0986e-02, -3.1189e-03,  1.6556e-03,  3.4932e-03,  4.7633e-03,\n",
       "                         -3.8655e-03, -1.0084e-02, -8.9470e-01, -1.1842e-02,  1.0919e-02,\n",
       "                          8.2574e-03,  3.0358e-03,  8.1782e-03, -4.9939e-03,  1.7624e-02,\n",
       "                          1.0873e-03, -7.5972e-03,  6.1289e-05, -5.8776e-03, -1.2747e-04,\n",
       "                         -1.0364e-02,  5.8151e-03,  3.0468e-03,  1.7920e-03,  4.2039e-03,\n",
       "                          9.5180e-03,  1.0376e-03,  7.1236e-03,  7.5341e-03, -8.4792e-03,\n",
       "                          1.3926e-02, -5.9579e-03,  2.9150e-03,  6.8736e-03, -8.7852e-03,\n",
       "                         -8.6532e-03,  9.8080e-03,  1.1254e-02,  4.6576e-03,  8.8945e-03,\n",
       "                          1.7058e-02,  2.3052e-03, -2.7348e-03, -9.8025e-05, -7.9816e-03,\n",
       "                          7.6487e-04, -2.0030e-03, -2.1567e-02, -5.6360e-03,  1.3181e-03,\n",
       "                          9.9111e-03, -4.3839e-03,  2.8152e-03, -3.1516e-03, -2.6136e-03,\n",
       "                         -8.5167e-03,  8.1660e-03,  1.1791e-02,  7.1137e-03, -1.5278e-03,\n",
       "                          2.2974e-03,  7.5099e-04,  2.9910e-02,  9.1808e-05, -9.7089e-03,\n",
       "                          5.9007e-04,  1.8295e-02, -2.1064e-03, -5.0225e-03,  2.8030e-03,\n",
       "                          8.0909e-03,  1.1848e-02,  7.7177e-03,  5.7540e-03, -2.3876e-03,\n",
       "                         -2.8140e-03, -1.1315e-02, -1.3093e-03,  2.6484e-03,  1.6757e-03,\n",
       "                         -5.4440e-03, -2.4921e-03, -3.1703e-04, -1.2210e-02, -6.4878e-03,\n",
       "                          3.8872e-03, -4.1208e-03, -8.9362e-04, -1.1852e-02, -2.5329e-03,\n",
       "                         -1.7963e-03, -9.6568e-04,  1.2525e-03, -4.1569e-03, -2.6921e-03,\n",
       "                          1.2960e-02, -1.7337e-03, -8.8916e-04,  1.6955e-03,  6.4417e-03,\n",
       "                         -3.6836e-03, -7.0451e-03, -8.1071e-02,  4.6106e-03,  3.0302e-03,\n",
       "                          3.5063e-03, -7.9768e-03,  9.8590e-03, -2.3657e-03, -5.2424e-03,\n",
       "                          1.6484e-03, -9.3588e-03, -5.6791e-03, -2.0417e-03, -1.0807e-02,\n",
       "                         -9.2044e-03,  8.3951e-03,  8.7919e-03, -6.4268e-03,  4.0202e-03,\n",
       "                         -4.2025e-03,  5.8163e-03, -5.2126e-03,  1.4110e-02,  7.2514e-03,\n",
       "                          1.0085e-02,  1.0862e-02, -6.2295e-03,  2.8106e-03, -5.8229e-03,\n",
       "                          5.6490e-04,  3.8948e-03, -1.3570e-02,  2.2753e-03, -5.8585e-03,\n",
       "                          5.8187e-03,  1.0474e-02, -5.4720e-03, -7.7038e-03, -5.5919e-03,\n",
       "                         -7.8044e-03, -1.0740e-02, -2.2916e-03, -1.1465e-03, -2.5533e-03,\n",
       "                         -2.2159e-03,  3.1646e-03,  3.7525e-03, -4.4344e-03, -9.1829e-03,\n",
       "                          6.4014e-03, -4.9437e-03,  5.3844e-03, -3.2939e-03, -9.0407e-03,\n",
       "                         -3.2248e-03, -6.0617e-03,  8.3787e-04, -9.8845e-03, -7.3132e-03,\n",
       "                         -4.3418e-03, -9.4770e-03,  7.8538e-03]]], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('word_embedding', Embedding(32000, 768)),\n",
       "              ('layer',\n",
       "               ModuleList(\n",
       "                 (0): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (1): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (2): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (3): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (4): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (5): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (6): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (7): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (8): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (9): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (10): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (11): XLNetLayer(\n",
       "                   (rel_attn): XLNetRelativeAttention(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (ff): XLNetFeedForward(\n",
       "                     (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "               )),\n",
       "              ('dropout', Dropout(p=0.1, inplace=False))]),\n",
       " 'config': XLNetConfig {\n",
       "   \"_name_or_path\": \"xlnet-base-cased\",\n",
       "   \"architectures\": [\n",
       "     \"XLNetLMHeadModel\"\n",
       "   ],\n",
       "   \"attn_type\": \"bi\",\n",
       "   \"bi_data\": false,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"clamp_len\": -1,\n",
       "   \"d_head\": 64,\n",
       "   \"d_inner\": 3072,\n",
       "   \"d_model\": 768,\n",
       "   \"dropout\": 0.1,\n",
       "   \"end_n_top\": 5,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"ff_activation\": \"gelu\",\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"mem_len\": null,\n",
       "   \"model_type\": \"xlnet\",\n",
       "   \"n_head\": 12,\n",
       "   \"n_layer\": 12,\n",
       "   \"pad_token_id\": 5,\n",
       "   \"reuse_len\": null,\n",
       "   \"same_length\": false,\n",
       "   \"start_n_top\": 5,\n",
       "   \"summary_activation\": \"tanh\",\n",
       "   \"summary_last_dropout\": 0.1,\n",
       "   \"summary_type\": \"last\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"task_specific_params\": {\n",
       "     \"text-generation\": {\n",
       "       \"do_sample\": true,\n",
       "       \"max_length\": 250\n",
       "     }\n",
       "   },\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"untie_r\": true,\n",
       "   \"use_mems_eval\": true,\n",
       "   \"use_mems_train\": false,\n",
       "   \"vocab_size\": 32000\n",
       " },\n",
       " 'name_or_path': 'xlnet-base-cased',\n",
       " 'mem_len': None,\n",
       " 'reuse_len': None,\n",
       " 'd_model': 768,\n",
       " 'same_length': False,\n",
       " 'attn_type': 'bi',\n",
       " 'bi_data': False,\n",
       " 'clamp_len': -1,\n",
       " 'n_layer': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1f9500-a841-44cc-bbbc-4c56aebf9f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('embeddings',\n",
       "               RobertaEmbeddings(\n",
       "                 (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "                 (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "                 (token_type_embeddings): Embedding(1, 768)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('encoder',\n",
       "               RobertaEncoder(\n",
       "                 (layer): ModuleList(\n",
       "                   (0): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (2): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (3): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (4): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (5): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (6): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (7): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (8): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (9): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (10): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (11): RobertaLayer(\n",
       "                     (attention): RobertaAttention(\n",
       "                       (self): RobertaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): RobertaSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): RobertaIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                     )\n",
       "                     (output): RobertaOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )),\n",
       "              ('pooler',\n",
       "               RobertaPooler(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (activation): Tanh()\n",
       "               ))]),\n",
       " 'config': RobertaConfig {\n",
       "   \"_name_or_path\": \"roberta-base\",\n",
       "   \"architectures\": [\n",
       "     \"RobertaForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-05,\n",
       "   \"max_position_embeddings\": 514,\n",
       "   \"model_type\": \"roberta\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50265\n",
       " },\n",
       " 'name_or_path': 'roberta-base'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb2031af-4f63-417a-b504-7d2bc4ca68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('embeddings',\n",
       "               ElectraEmbeddings(\n",
       "                 (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "                 (position_embeddings): Embedding(512, 768)\n",
       "                 (token_type_embeddings): Embedding(2, 768)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('embeddings_project',\n",
       "               Linear(in_features=768, out_features=256, bias=True)),\n",
       "              ('encoder',\n",
       "               ElectraEncoder(\n",
       "                 (layer): ModuleList(\n",
       "                   (0): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (2): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (3): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (4): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (5): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (6): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (7): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (8): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (9): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (10): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (11): ElectraLayer(\n",
       "                     (attention): ElectraAttention(\n",
       "                       (self): ElectraSelfAttention(\n",
       "                         (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): ElectraSelfOutput(\n",
       "                         (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                         (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): ElectraIntermediate(\n",
       "                       (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     )\n",
       "                     (output): ElectraOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                       (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               ))]),\n",
       " 'config': ElectraConfig {\n",
       "   \"_name_or_path\": \"google/electra-base-generator\",\n",
       "   \"architectures\": [\n",
       "     \"ElectraForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"embedding_size\": 768,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 256,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 1024,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 512,\n",
       "   \"model_type\": \"electra\",\n",
       "   \"num_attention_heads\": 4,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"summary_activation\": \"gelu\",\n",
       "   \"summary_last_dropout\": 0.1,\n",
       "   \"summary_type\": \"first\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"type_vocab_size\": 2,\n",
       "   \"vocab_size\": 30522\n",
       " },\n",
       " 'name_or_path': 'google/electra-base-generator'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_electra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2a3f43-3900-4cf5-a043-1490fcd80bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('embeddings',\n",
       "               Embeddings(\n",
       "                 (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "                 (position_embeddings): Embedding(512, 768)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('transformer',\n",
       "               Transformer(\n",
       "                 (layer): ModuleList(\n",
       "                   (0): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                   (1): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                   (2): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                   (3): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                   (4): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                   (5): TransformerBlock(\n",
       "                     (attention): MultiHeadSelfAttention(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                     (ffn): FFN(\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                     )\n",
       "                     (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   )\n",
       "                 )\n",
       "               ))]),\n",
       " 'config': DistilBertConfig {\n",
       "   \"_name_or_path\": \"distilbert-base-cased\",\n",
       "   \"activation\": \"gelu\",\n",
       "   \"attention_dropout\": 0.1,\n",
       "   \"dim\": 768,\n",
       "   \"dropout\": 0.1,\n",
       "   \"hidden_dim\": 3072,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"max_position_embeddings\": 512,\n",
       "   \"model_type\": \"distilbert\",\n",
       "   \"n_heads\": 12,\n",
       "   \"n_layers\": 6,\n",
       "   \"output_past\": true,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"qa_dropout\": 0.1,\n",
       "   \"seq_classif_dropout\": 0.2,\n",
       "   \"sinusoidal_pos_embds\": false,\n",
       "   \"tie_weights_\": true,\n",
       "   \"transformers_version\": \"4.6.0\",\n",
       "   \"vocab_size\": 28996\n",
       " },\n",
       " 'name_or_path': 'distilbert-base-cased'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d013c9-d637-4a99-a7a3-56f93b639fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_enc = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_dec = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3272d-c900-4662-9386-aeac2601e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d6746d2-1f7f-4eba-a042-0ac88e235263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('encoder',\n",
       "               BertModel(\n",
       "                 (embeddings): BertEmbeddings(\n",
       "                   (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "                   (position_embeddings): Embedding(512, 768)\n",
       "                   (token_type_embeddings): Embedding(2, 768)\n",
       "                   (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                   (dropout): Dropout(p=0.1, inplace=False)\n",
       "                 )\n",
       "                 (encoder): BertEncoder(\n",
       "                   (layer): ModuleList(\n",
       "                     (0): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (1): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (2): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (3): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (4): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (5): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (6): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (7): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (8): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (9): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (10): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (11): BertLayer(\n",
       "                       (attention): BertAttention(\n",
       "                         (self): BertSelfAttention(\n",
       "                           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                         (output): BertSelfOutput(\n",
       "                           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                           (dropout): Dropout(p=0.1, inplace=False)\n",
       "                         )\n",
       "                       )\n",
       "                       (intermediate): BertIntermediate(\n",
       "                         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       )\n",
       "                       (output): BertOutput(\n",
       "                         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (pooler): BertPooler(\n",
       "                   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                   (activation): Tanh()\n",
       "                 )\n",
       "               )),\n",
       "              ('decoder',\n",
       "               GPT2LMHeadModel(\n",
       "                 (transformer): GPT2Model(\n",
       "                   (wte): Embedding(50257, 768)\n",
       "                   (wpe): Embedding(1024, 768)\n",
       "                   (drop): Dropout(p=0.1, inplace=False)\n",
       "                   (h): ModuleList(\n",
       "                     (0): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (1): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (2): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (3): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (4): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (5): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (6): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (7): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (8): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (9): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (10): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (11): GPT2Block(\n",
       "                       (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (attn): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (crossattention): GPT2Attention(\n",
       "                         (c_attn): Conv1D()\n",
       "                         (q_attn): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                       (mlp): GPT2MLP(\n",
       "                         (c_fc): Conv1D()\n",
       "                         (c_proj): Conv1D()\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                   )\n",
       "                   (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                 )\n",
       "                 (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "               ))]),\n",
       " 'config': EncoderDecoderConfig {\n",
       "   \"decoder\": {\n",
       "     \"_name_or_path\": \"gpt2\",\n",
       "     \"activation_function\": \"gelu_new\",\n",
       "     \"add_cross_attention\": true,\n",
       "     \"architectures\": [\n",
       "       \"GPT2LMHeadModel\"\n",
       "     ],\n",
       "     \"attn_pdrop\": 0.1,\n",
       "     \"bad_words_ids\": null,\n",
       "     \"bos_token_id\": 50256,\n",
       "     \"chunk_size_feed_forward\": 0,\n",
       "     \"decoder_start_token_id\": null,\n",
       "     \"diversity_penalty\": 0.0,\n",
       "     \"do_sample\": false,\n",
       "     \"early_stopping\": false,\n",
       "     \"embd_pdrop\": 0.1,\n",
       "     \"encoder_no_repeat_ngram_size\": 0,\n",
       "     \"eos_token_id\": 50256,\n",
       "     \"finetuning_task\": null,\n",
       "     \"forced_bos_token_id\": null,\n",
       "     \"forced_eos_token_id\": null,\n",
       "     \"gradient_checkpointing\": false,\n",
       "     \"id2label\": {\n",
       "       \"0\": \"LABEL_0\",\n",
       "       \"1\": \"LABEL_1\"\n",
       "     },\n",
       "     \"initializer_range\": 0.02,\n",
       "     \"is_decoder\": true,\n",
       "     \"is_encoder_decoder\": false,\n",
       "     \"label2id\": {\n",
       "       \"LABEL_0\": 0,\n",
       "       \"LABEL_1\": 1\n",
       "     },\n",
       "     \"layer_norm_epsilon\": 1e-05,\n",
       "     \"length_penalty\": 1.0,\n",
       "     \"max_length\": 20,\n",
       "     \"min_length\": 0,\n",
       "     \"model_type\": \"gpt2\",\n",
       "     \"n_ctx\": 1024,\n",
       "     \"n_embd\": 768,\n",
       "     \"n_head\": 12,\n",
       "     \"n_inner\": null,\n",
       "     \"n_layer\": 12,\n",
       "     \"n_positions\": 1024,\n",
       "     \"no_repeat_ngram_size\": 0,\n",
       "     \"num_beam_groups\": 1,\n",
       "     \"num_beams\": 1,\n",
       "     \"num_return_sequences\": 1,\n",
       "     \"output_attentions\": false,\n",
       "     \"output_hidden_states\": false,\n",
       "     \"output_scores\": false,\n",
       "     \"pad_token_id\": null,\n",
       "     \"prefix\": null,\n",
       "     \"problem_type\": null,\n",
       "     \"pruned_heads\": {},\n",
       "     \"remove_invalid_values\": false,\n",
       "     \"repetition_penalty\": 1.0,\n",
       "     \"resid_pdrop\": 0.1,\n",
       "     \"return_dict\": true,\n",
       "     \"return_dict_in_generate\": false,\n",
       "     \"scale_attn_weights\": true,\n",
       "     \"sep_token_id\": null,\n",
       "     \"summary_activation\": null,\n",
       "     \"summary_first_dropout\": 0.1,\n",
       "     \"summary_proj_to_labels\": true,\n",
       "     \"summary_type\": \"cls_index\",\n",
       "     \"summary_use_proj\": true,\n",
       "     \"task_specific_params\": {\n",
       "       \"text-generation\": {\n",
       "         \"do_sample\": true,\n",
       "         \"max_length\": 50\n",
       "       }\n",
       "     },\n",
       "     \"temperature\": 1.0,\n",
       "     \"tie_encoder_decoder\": false,\n",
       "     \"tie_word_embeddings\": true,\n",
       "     \"tokenizer_class\": null,\n",
       "     \"top_k\": 50,\n",
       "     \"top_p\": 1.0,\n",
       "     \"torchscript\": false,\n",
       "     \"transformers_version\": \"4.6.0\",\n",
       "     \"use_bfloat16\": false,\n",
       "     \"use_cache\": true,\n",
       "     \"vocab_size\": 50257\n",
       "   },\n",
       "   \"encoder\": {\n",
       "     \"_name_or_path\": \"bert-base-uncased\",\n",
       "     \"add_cross_attention\": false,\n",
       "     \"architectures\": [\n",
       "       \"BertForMaskedLM\"\n",
       "     ],\n",
       "     \"attention_probs_dropout_prob\": 0.1,\n",
       "     \"bad_words_ids\": null,\n",
       "     \"bos_token_id\": null,\n",
       "     \"chunk_size_feed_forward\": 0,\n",
       "     \"decoder_start_token_id\": null,\n",
       "     \"diversity_penalty\": 0.0,\n",
       "     \"do_sample\": false,\n",
       "     \"early_stopping\": false,\n",
       "     \"encoder_no_repeat_ngram_size\": 0,\n",
       "     \"eos_token_id\": null,\n",
       "     \"finetuning_task\": null,\n",
       "     \"forced_bos_token_id\": null,\n",
       "     \"forced_eos_token_id\": null,\n",
       "     \"gradient_checkpointing\": false,\n",
       "     \"hidden_act\": \"gelu\",\n",
       "     \"hidden_dropout_prob\": 0.1,\n",
       "     \"hidden_size\": 768,\n",
       "     \"id2label\": {\n",
       "       \"0\": \"LABEL_0\",\n",
       "       \"1\": \"LABEL_1\"\n",
       "     },\n",
       "     \"initializer_range\": 0.02,\n",
       "     \"intermediate_size\": 3072,\n",
       "     \"is_decoder\": false,\n",
       "     \"is_encoder_decoder\": false,\n",
       "     \"label2id\": {\n",
       "       \"LABEL_0\": 0,\n",
       "       \"LABEL_1\": 1\n",
       "     },\n",
       "     \"layer_norm_eps\": 1e-12,\n",
       "     \"length_penalty\": 1.0,\n",
       "     \"max_length\": 20,\n",
       "     \"max_position_embeddings\": 512,\n",
       "     \"min_length\": 0,\n",
       "     \"model_type\": \"bert\",\n",
       "     \"no_repeat_ngram_size\": 0,\n",
       "     \"num_attention_heads\": 12,\n",
       "     \"num_beam_groups\": 1,\n",
       "     \"num_beams\": 1,\n",
       "     \"num_hidden_layers\": 12,\n",
       "     \"num_return_sequences\": 1,\n",
       "     \"output_attentions\": false,\n",
       "     \"output_hidden_states\": false,\n",
       "     \"output_scores\": false,\n",
       "     \"pad_token_id\": 0,\n",
       "     \"position_embedding_type\": \"absolute\",\n",
       "     \"prefix\": null,\n",
       "     \"problem_type\": null,\n",
       "     \"pruned_heads\": {},\n",
       "     \"remove_invalid_values\": false,\n",
       "     \"repetition_penalty\": 1.0,\n",
       "     \"return_dict\": true,\n",
       "     \"return_dict_in_generate\": false,\n",
       "     \"sep_token_id\": null,\n",
       "     \"task_specific_params\": null,\n",
       "     \"temperature\": 1.0,\n",
       "     \"tie_encoder_decoder\": false,\n",
       "     \"tie_word_embeddings\": true,\n",
       "     \"tokenizer_class\": null,\n",
       "     \"top_k\": 50,\n",
       "     \"top_p\": 1.0,\n",
       "     \"torchscript\": false,\n",
       "     \"transformers_version\": \"4.6.0\",\n",
       "     \"type_vocab_size\": 2,\n",
       "     \"use_bfloat16\": false,\n",
       "     \"use_cache\": true,\n",
       "     \"vocab_size\": 30522\n",
       "   },\n",
       "   \"is_encoder_decoder\": true,\n",
       "   \"model_type\": \"encoder-decoder\",\n",
       "   \"transformers_version\": null\n",
       " },\n",
       " 'name_or_path': ''}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37732b3f-b0b6-4ee0-ad5d-c9999575aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sodium hypochlorite ( commonly known in a dilute solution as bleach ) is an inorganic chemical compound with the formula naocl ( or naclo ), comprising a sodium cation ( na + ) and a hypochlorite anion ( ocl− or clo− ).']\n",
      "['\\nThe first time I saw the new iPhone 6, I was excited. I was excited to']\n"
     ]
    }
   ],
   "source": [
    "inputs22 = tokenizer_enc(interesting_text_1, return_tensors=\"pt\")\n",
    "r22 = model_test.generate(inputs22.input_ids)\n",
    "print(tokenizer_enc.batch_decode(inputs22.input_ids, skip_special_tokens=True))\n",
    "print(tokenizer_dec.batch_decode(r22, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "725233aa-4aeb-4462-bcb9-67a5edb17f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sodium hypochlorite (commonly known in a dilute solution as bleach) is an inorganic chemical compound with the formula NaOCl (or NaClO),  comprising a sodium cation (Na+) and a hypchlorite anion (OCl+)\n"
     ]
    }
   ],
   "source": [
    "inputs33 = tokenizer_bart(interesting_text_1, return_tensors=\"pt\")\n",
    "r33 = model_bart.generate(inputs33.input_ids, num_beams=4, length_penalty=2.0, no_repeat_ngram_size=3)\n",
    "print(tokenizer_bart.decode(r33.squeeze(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab551086-68bc-4846-9ef0-ddfe516b4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder(model, tokenizer, text):\n",
    "    if model == None or tokenizer == None:\n",
    "        return\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    #print(inputs)\n",
    "    outputs = model(**inputs)\n",
    "    print(f'\\033[1mTokenizer name: {tokenizer.name_or_path} vocab_size: {tokenizer.vocab_size}\\033[0m')\n",
    "    print(f'\\033[1mEncoder name: {model.name_or_path} \\033[0m')\n",
    "    # print(f'Num tokens: {len(t.input_ids)}')\n",
    "    print('-' * 180)\n",
    "    print(outputs)\n",
    "    # print(f'Tokens      text: {text}')\n",
    "    # print('-' * 220)\n",
    "    # print(f'Tokens input_ids: {t.input_ids}')\n",
    "    print('=' * 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce3b4e5d-407c-4c00-9dae-56c7f996ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_info(text):\n",
    "    print(f'Text: {text}')\n",
    "    print('=' * 180)\n",
    "    #print_sentence_statistics(text)\n",
    "    #print('=' * 180)\n",
    "    model_lists = [model_bert, None, model_xlnet, model_roberta, model_electra, model_distilbert]\n",
    "    tok_lists = [tokenizer_bert, tokenizer_bart, tokenizer_xlnet, tokenizer_roberta, tokenizer_electra, tokenizer_distilbert]\n",
    "    for model, tok in zip(model_lists, tok_lists):\n",
    "        test_encoder(model, tok, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9caf2-6127-4468-a296-b41d0bcdef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_info(interesting_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572f308-14e6-42f1-bacc-09a2a1fda706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
